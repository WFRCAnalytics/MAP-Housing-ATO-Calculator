{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union with DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the necessary datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.validation import make_valid\n",
    "from shapely import set_precision\n",
    "# from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import _params as params\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 26912\n",
    "GRID = 0.1 # meters in EPSG:26912; 0.1â€“1.0 works well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate(file_path):\n",
    "    \"\"\"Load a file, ensure CRS, fix invalid/empty geometries.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Missing expected layer: {file_path}\")\n",
    "    gdf = gpd.read_file(file_path)\n",
    "    if gdf.empty:\n",
    "        raise ValueError(f\"Layer has no features: {file_path}\")\n",
    "\n",
    "    # normalize CRS to WGS84 then project to target for overlay robustness\n",
    "    if gdf.crs is None:\n",
    "        # assume WGS84 if unspecified (adjust if your sources differ)\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "    gdf = gdf.to_crs(epsg=TARGET_CRS)\n",
    "    gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.notna()].copy()\n",
    "\n",
    "    # Fix any invalid geometries\n",
    "    invalid_count = (~gdf.is_valid).sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\"   ğŸ”§ Fixing {invalid_count} invalid geometries\")\n",
    "        gdf[\"geometry\"] = gdf[\"geometry\"].apply(make_valid)\n",
    "        gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda g: set_precision(g, GRID))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_multipolygon(gdf):\n",
    "    \"\"\"Convert all geometries to MultiPolygon, filtering out non-polygon types.\"\"\"\n",
    "    def _to_multipolygon(geom):\n",
    "        if geom is None:\n",
    "            return None\n",
    "        if geom.geom_type == \"Polygon\":\n",
    "            return MultiPolygon([geom])\n",
    "        elif geom.geom_type == \"MultiPolygon\":\n",
    "            return geom\n",
    "        else:\n",
    "            # Drop non-polygon geometries (points, lines, etc.)\n",
    "            return None\n",
    "\n",
    "    gdf = gdf.copy()\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(_to_multipolygon)\n",
    "    gdf = gdf[gdf.geometry.notna() & ~gdf.geometry.is_empty].copy()\n",
    "\n",
    "    # Verify all geometries are MultiPolygon\n",
    "    if not gdf.empty and not all(gdf.geometry.geom_type == 'MultiPolygon'):\n",
    "        print(f\"Warning: Not all geometries converted to MultiPolygon\")\n",
    "        gdf = gdf[gdf.geometry.geom_type == 'MultiPolygon'].copy()\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_buffer_suffix(file_path):\n",
    "    \"\"\"Add '_Buffers' to filename before extension.\"\"\"\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    return f\"{base}_Buffers{ext}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that will be processed as-is (no buffering needed)\n",
    "regular_files = [\n",
    "    params.strCommunitiesOut,\n",
    "    params.strCentersOut,\n",
    "    params.strTAZwithATOOut,\n",
    "    params.strQOZOut,\n",
    "    params.strParkAccessibilityOut\n",
    "]\n",
    "\n",
    "# Files that need buffering (points/lines â†’ polygons)\n",
    "files_to_buffer = [\n",
    "    params.strInterchangesOut_Cur,\n",
    "    params.strInterchangesOut_Fut,\n",
    "    params.strTransitOut_Lcl,\n",
    "    params.strTransitOut_BrtCur,\n",
    "    params.strTransitOut_BrtFut,\n",
    "    params.strTransitOut_LrtCur,\n",
    "    params.strTransitOut_LrtFut,\n",
    "    params.strTransitOut_CrtCur,\n",
    "    params.strTransitOut_CrtFut,\n",
    "    params.strChildCareOut,\n",
    "    params.strHealthCareOut,\n",
    "    params.strSchoolsOut_RegPub,\n",
    "    params.strSchoolsOut_HighEd,\n",
    "    params.strGroceryOut,\n",
    "    params.strCommunityCenterOut,\n",
    "    params.strPathsOut_Cur,\n",
    "    params.strPathsOut_Fut,\n",
    "    params.strATCycleTracksOut_Cur,\n",
    "    params.strATCycleTracksOut_Fut\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiatize all layers\n",
    "all_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading regular files...\n",
      "âœ… Loaded Communities (75 MultiPolygons)\n",
      "âœ… Loaded Centers (351 MultiPolygons)\n",
      "âœ… Loaded TAZWithATOScores (2858 MultiPolygons)\n",
      "âœ… Loaded UtahQualifiedOpportunityZones (28 MultiPolygons)\n",
      "   ğŸ”§ Fixing 1 invalid geometries\n",
      "âœ… Loaded ParksAndOpenSpace (1 MultiPolygons)\n"
     ]
    }
   ],
   "source": [
    "# Load regular files\n",
    "print(\"Loading regular files...\")\n",
    "for file_path in regular_files:\n",
    "    try:\n",
    "        name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        gdf = load_and_validate(file_path)\n",
    "        gdf = ensure_multipolygon(gdf)  # Convert to MultiPolygon in one step\n",
    "        all_layers[name] = gdf\n",
    "        print(f\"âœ… Loaded {name} ({len(gdf)} MultiPolygons)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing {file_path}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading buffered files...\n",
      "âœ… Loaded Interchanges_Buffers (984 MultiPolygons)\n",
      "âœ… Loaded Interchanges_Future_Buffers (96 MultiPolygons)\n",
      "âœ… Loaded LocalBusStops_Buffers (5299 MultiPolygons)\n",
      "âœ… Loaded BRTStops_Buffers (40 MultiPolygons)\n",
      "âœ… Loaded BRTStops_Future_Buffers (118 MultiPolygons)\n",
      "âœ… Loaded LRTStops_Buffers (56 MultiPolygons)\n",
      "âœ… Loaded LRTStops_Future_Buffers (73 MultiPolygons)\n",
      "âœ… Loaded CRTStops_Buffers (15 MultiPolygons)\n",
      "âœ… Loaded CRTStops_Future_Buffers (9 MultiPolygons)\n",
      "âœ… Loaded ChildCare_Buffers (740 MultiPolygons)\n",
      "âœ… Loaded HealthCare_Buffers (472 MultiPolygons)\n",
      "âœ… Loaded SchoolsRegPublic_Buffers (2060 MultiPolygons)\n",
      "âœ… Loaded SchoolsHigherEd_Buffers (130 MultiPolygons)\n",
      "âœ… Loaded GroceryStores_Buffers (874 MultiPolygons)\n",
      "âœ… Loaded CommunityCenter_Buffers (350 MultiPolygons)\n",
      "âœ… Loaded ATPaths_Buffers (16064 MultiPolygons)\n",
      "âœ… Loaded ATPaths_Future_Buffers (1378 MultiPolygons)\n",
      "âœ… Loaded ATCycleTracks_Buffers (90 MultiPolygons)\n",
      "âœ… Loaded ATCycleTracks_Future_Buffers (1574 MultiPolygons)\n"
     ]
    }
   ],
   "source": [
    "# Load buffered files\n",
    "print(\"\\nLoading buffered files...\")\n",
    "for file_path in files_to_buffer:\n",
    "    try:\n",
    "        buffered_path = add_buffer_suffix(file_path)\n",
    "        name = os.path.splitext(os.path.basename(buffered_path))[0]\n",
    "        gdf = load_and_validate(buffered_path)\n",
    "        gdf = ensure_multipolygon(gdf)  # Convert to MultiPolygon in one step\n",
    "        all_layers[name] = gdf\n",
    "        print(f\"âœ… Loaded {name} ({len(gdf)} MultiPolygons)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing {file_path}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading parcel layer...\n",
      "   ğŸ”§ Fixing 358 invalid geometries\n",
      "âœ… Loaded Parcels (712179 MultiPolygons)\n",
      "\n",
      "ğŸ¯ Total layers ready for union: 24 + Parcels\n"
     ]
    }
   ],
   "source": [
    "# Load parcel layer separately\n",
    "print(\"\\nLoading parcel layer...\")\n",
    "try:\n",
    "    parcels = load_and_validate(params.strParcelsOut)\n",
    "    parcels = ensure_multipolygon(parcels)  # Convert to MultiPolygon in one step\n",
    "    print(f\"âœ… Loaded Parcels ({len(parcels)} MultiPolygons)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error loading parcels: {e}\")\n",
    "    parcels = None\n",
    "\n",
    "print(f\"\\nğŸ¯ Total layers ready for union: {len(all_layers)} + Parcels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Union Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_geometry_for_overlay(gdf, buffer_dist=0.0001):\n",
    "    \"\"\"Clean and prepare geometry for overlay operations.\"\"\"\n",
    "    # Remove empty/null geometries\n",
    "    gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.notna()].copy()\n",
    "\n",
    "    # Fix invalid geometries\n",
    "    invalid_mask = ~gdf.geometry.is_valid\n",
    "    if invalid_mask.any():\n",
    "        print(f\"   Fixing {invalid_mask.sum()} invalid geometries\")\n",
    "        gdf.loc[invalid_mask, 'geometry'] = gdf.loc[invalid_mask, 'geometry'].apply(make_valid)\n",
    "\n",
    "    # Apply tiny buffer to fix topology issues\n",
    "    gdf['geometry'] = gdf.geometry.buffer(buffer_dist)\n",
    "\n",
    "    # Force back to MultiPolygon after buffer (buffer can change geometry types)\n",
    "    gdf = ensure_multipolygon(gdf)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_all_layers_simple(layers_dict, parcels_gdf):\n",
    "    \"\"\"\n",
    "    Simple union approach: Use intersections instead of complex overlay operations.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Starting simple union operation...\")\n",
    "\n",
    "    if parcels_gdf is None or parcels_gdf.empty:\n",
    "        raise ValueError(\"Parcels layer is required and cannot be empty\")\n",
    "\n",
    "    # Start with clean parcels\n",
    "    print(\"ğŸ“¦ Preparing parcels as base layer...\")\n",
    "    result_gdf = clean_geometry_for_overlay(parcels_gdf.copy())\n",
    "    print(f\"   Parcels prepared: {len(result_gdf)} features\")\n",
    "\n",
    "    # Add indicator columns for all layers\n",
    "    for layer_name in layers_dict.keys():\n",
    "        result_gdf[f'has_{layer_name}'] = 0\n",
    "\n",
    "    # Process each layer by finding intersections\n",
    "    layer_count = 0\n",
    "    total_layers = len(layers_dict)\n",
    "\n",
    "    for layer_name, layer_gdf in layers_dict.items():\n",
    "        layer_count += 1\n",
    "        print(f\"ğŸ”„ Processing layer {layer_count}/{total_layers}: {layer_name} ({len(layer_gdf)} features)\")\n",
    "\n",
    "        if layer_gdf.empty:\n",
    "            print(f\"   Skipping empty layer: {layer_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Clean the overlay layer\n",
    "            overlay_gdf = clean_geometry_for_overlay(layer_gdf.copy())\n",
    "            print(f\"   Cleaned overlay layer: {len(overlay_gdf)} features\")\n",
    "\n",
    "            # Ensure same CRS\n",
    "            if overlay_gdf.crs != result_gdf.crs:\n",
    "                overlay_gdf = overlay_gdf.to_crs(result_gdf.crs)\n",
    "\n",
    "            # Create spatial index for efficiency\n",
    "            spatial_index = overlay_gdf.sindex\n",
    "\n",
    "            # Find intersecting parcels\n",
    "            intersecting_indices = []\n",
    "            for idx, parcel_geom in result_gdf.geometry.items():\n",
    "                possible_matches_index = list(spatial_index.intersection(parcel_geom.bounds))\n",
    "                if possible_matches_index:\n",
    "                    for match_idx in possible_matches_index:\n",
    "                        if parcel_geom.intersects(overlay_gdf.iloc[match_idx].geometry):\n",
    "                            intersecting_indices.append(idx)\n",
    "                            break\n",
    "\n",
    "            # Mark intersecting parcels\n",
    "            result_gdf.loc[intersecting_indices, f'has_{layer_name}'] = 1\n",
    "\n",
    "            print(f\"   Found {len(intersecting_indices)} intersecting parcels\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš  Error processing {layer_name}: {e}\")\n",
    "            print(\"   Continuing with remaining layers...\")\n",
    "            continue\n",
    "\n",
    "    print(f\"âœ… Simple union complete! Final result: {len(result_gdf)} features\")\n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Union Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Starting simple union operation...\n",
      "ğŸ“¦ Preparing parcels as base layer...\n",
      "   Parcels prepared: 710220 features\n",
      "ğŸ”„ Processing layer 1/24: Communities (75 features)\n",
      "   Cleaned overlay layer: 75 features\n",
      "   Found 671830 intersecting parcels\n",
      "ğŸ”„ Processing layer 2/24: Centers (351 features)\n",
      "   Cleaned overlay layer: 351 features\n",
      "   Found 46356 intersecting parcels\n",
      "ğŸ”„ Processing layer 3/24: TAZWithATOScores (2858 features)\n",
      "   Cleaned overlay layer: 2858 features\n",
      "   Found 706586 intersecting parcels\n",
      "ğŸ”„ Processing layer 4/24: UtahQualifiedOpportunityZones (28 features)\n",
      "   Cleaned overlay layer: 28 features\n",
      "   Found 34067 intersecting parcels\n",
      "ğŸ”„ Processing layer 5/24: ParksAndOpenSpace (1 features)\n",
      "   Cleaned overlay layer: 1 features\n",
      "   Found 327415 intersecting parcels\n",
      "ğŸ”„ Processing layer 6/24: Interchanges_Buffers (984 features)\n",
      "   Cleaned overlay layer: 984 features\n",
      "   Found 400979 intersecting parcels\n",
      "ğŸ”„ Processing layer 7/24: Interchanges_Future_Buffers (96 features)\n",
      "   Cleaned overlay layer: 96 features\n",
      "   Found 159352 intersecting parcels\n",
      "ğŸ”„ Processing layer 8/24: LocalBusStops_Buffers (5299 features)\n",
      "   Cleaned overlay layer: 5299 features\n",
      "   Found 262144 intersecting parcels\n",
      "ğŸ”„ Processing layer 9/24: BRTStops_Buffers (40 features)\n",
      "   Cleaned overlay layer: 40 features\n",
      "   Found 12007 intersecting parcels\n",
      "ğŸ”„ Processing layer 10/24: BRTStops_Future_Buffers (118 features)\n",
      "   Cleaned overlay layer: 118 features\n",
      "   Found 36185 intersecting parcels\n",
      "ğŸ”„ Processing layer 11/24: LRTStops_Buffers (56 features)\n",
      "   Cleaned overlay layer: 56 features\n",
      "   Found 14589 intersecting parcels\n",
      "ğŸ”„ Processing layer 12/24: LRTStops_Future_Buffers (73 features)\n",
      "   Cleaned overlay layer: 73 features\n",
      "   Found 27524 intersecting parcels\n",
      "ğŸ”„ Processing layer 13/24: CRTStops_Buffers (15 features)\n",
      "   Cleaned overlay layer: 15 features\n",
      "   Found 3607 intersecting parcels\n",
      "ğŸ”„ Processing layer 14/24: CRTStops_Future_Buffers (9 features)\n",
      "   Cleaned overlay layer: 9 features\n",
      "   Found 1857 intersecting parcels\n",
      "ğŸ”„ Processing layer 15/24: ChildCare_Buffers (740 features)\n",
      "   Cleaned overlay layer: 740 features\n",
      "   Found 425340 intersecting parcels\n",
      "ğŸ”„ Processing layer 16/24: HealthCare_Buffers (472 features)\n",
      "   Cleaned overlay layer: 472 features\n",
      "   Found 274713 intersecting parcels\n",
      "ğŸ”„ Processing layer 17/24: SchoolsRegPublic_Buffers (2060 features)\n",
      "   Cleaned overlay layer: 2060 features\n",
      "   Found 570476 intersecting parcels\n",
      "ğŸ”„ Processing layer 18/24: SchoolsHigherEd_Buffers (130 features)\n",
      "   Cleaned overlay layer: 130 features\n",
      "   Found 157249 intersecting parcels\n",
      "ğŸ”„ Processing layer 19/24: GroceryStores_Buffers (874 features)\n",
      "   Cleaned overlay layer: 874 features\n",
      "   Found 441296 intersecting parcels\n",
      "ğŸ”„ Processing layer 20/24: CommunityCenter_Buffers (350 features)\n",
      "   Cleaned overlay layer: 350 features\n",
      "   Found 314687 intersecting parcels\n",
      "ğŸ”„ Processing layer 21/24: ATPaths_Buffers (16064 features)\n",
      "   Cleaned overlay layer: 16064 features\n",
      "   Found 586036 intersecting parcels\n",
      "ğŸ”„ Processing layer 22/24: ATPaths_Future_Buffers (1378 features)\n",
      "   Cleaned overlay layer: 1378 features\n",
      "   Found 343376 intersecting parcels\n",
      "ğŸ”„ Processing layer 23/24: ATCycleTracks_Buffers (90 features)\n",
      "   Cleaned overlay layer: 90 features\n",
      "   Found 21301 intersecting parcels\n",
      "ğŸ”„ Processing layer 24/24: ATCycleTracks_Future_Buffers (1574 features)\n",
      "   Cleaned overlay layer: 1574 features\n",
      "   Found 190464 intersecting parcels\n",
      "âœ… Simple union complete! Final result: 710220 features\n",
      "\n",
      "ğŸ“Š Union Summary:\n",
      "   Total output features: 710,220\n",
      "   Average layers per feature: 8.49\n",
      "   Max layers in single feature: 22\n",
      "   Layer distribution:\n",
      "     0 layers: 2,316 parcels\n",
      "     1 layers: 23,982 parcels\n",
      "     2 layers: 15,839 parcels\n",
      "     3 layers: 23,454 parcels\n",
      "     4 layers: 40,289 parcels\n",
      "     5 layers: 52,505 parcels\n",
      "     6 layers: 55,874 parcels\n",
      "     7 layers: 52,808 parcels\n",
      "     8 layers: 61,174 parcels\n",
      "     9 layers: 77,513 parcels\n"
     ]
    }
   ],
   "source": [
    "if parcels is not None and not parcels.empty:\n",
    "    try:\n",
    "        # Perform the union operation\n",
    "        gdf_unioned = union_all_layers_simple(all_layers, parcels)\n",
    "\n",
    "        # Add summary statistics\n",
    "        layer_columns = [col for col in gdf_unioned.columns if col.startswith('has_')]\n",
    "        gdf_unioned['total_layers'] = gdf_unioned[layer_columns].sum(axis=1)\n",
    "\n",
    "        print(f\"\\nğŸ“Š Union Summary:\")\n",
    "        print(f\"   Total output features: {len(gdf_unioned):,}\")\n",
    "        print(f\"   Average layers per feature: {gdf_unioned['total_layers'].mean():.2f}\")\n",
    "        print(f\"   Max layers in single feature: {gdf_unioned['total_layers'].max()}\")\n",
    "\n",
    "        # Show distribution of layer counts\n",
    "        layer_dist = gdf_unioned['total_layers'].value_counts().sort_index()\n",
    "        print(f\"   Layer distribution:\")\n",
    "        for layers, count in layer_dist.head(10).items():\n",
    "            print(f\"     {int(layers)} layers: {count:,} parcels\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Union operation failed: {e}\")\n",
    "        gdf_unioned = None\n",
    "else:\n",
    "    print(\"âŒ Cannot perform union: Parcels layer is missing or empty\") # 1736687 esimated total # Run Time ~ 182m 33.9s\n",
    "    gdf_unioned = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Exporting results...\n",
      "âœ… Union result exported to: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\UNIONED.geojson\n",
      "   File size: 4363.0 MB\n",
      "   Sample (first 1000 features) exported to: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\UNIONED_sample.geojson\n"
     ]
    }
   ],
   "source": [
    "if gdf_unioned is not None and not gdf_unioned.empty:\n",
    "    print(\"\\nğŸ’¾ Exporting results...\")\n",
    "\n",
    "    # Reproject to WGS84 for export\n",
    "    gdf_unioned = gdf_unioned.to_crs(epsg=4326)\n",
    "\n",
    "    # Export to GeoJSON\n",
    "    output_path = os.path.join(params.dirIntermediate, \"UNIONED.geojson\")\n",
    "\n",
    "    try:\n",
    "        gdf_unioned.to_file(output_path, driver=\"GeoJSON\")\n",
    "        print(f\"âœ… Union result exported to: {output_path}\")\n",
    "        print(f\"   File size: {os.path.getsize(output_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "        # Also export a sample for inspection\n",
    "        sample_path = os.path.join(params.dirIntermediate, \"UNIONED_sample.geojson\")\n",
    "        gdf_unioned.head(1000).to_file(sample_path, driver=\"GeoJSON\")\n",
    "        print(f\"   Sample (first 1000 features) exported to: {sample_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Export failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ No results to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
