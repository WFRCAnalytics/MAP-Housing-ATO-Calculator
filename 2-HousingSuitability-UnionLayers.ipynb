{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union with DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the necessary datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.validation import make_valid\n",
    "import _params as params\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 26912\n",
    "CHUNK_SIZE = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate(file_path):\n",
    "    \"\"\"Load file, reproject to target CRS, and fix invalid geometries.\"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    print(f\"📂 Loading: {filename}\")\n",
    "\n",
    "    # Load and reproject\n",
    "    gdf = gpd.read_file(file_path).to_crs(epsg=TARGET_CRS)\n",
    "\n",
    "    # Fix any invalid geometries\n",
    "    invalid_count = (~gdf.is_valid).sum()\n",
    "    if invalid_count > 0:\n",
    "        print(f\"   🔧 Fixing {invalid_count} invalid geometries\")\n",
    "        gdf[\"geometry\"] = gdf[\"geometry\"].apply(make_valid)\n",
    "\n",
    "    print(f\"   ✅ {len(gdf)} features loaded\")\n",
    "    return gdf\n",
    "\n",
    "def add_buffer_suffix(file_path):\n",
    "    \"\"\"Add '_Buffers' to filename before extension.\"\"\"\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    return f\"{base}_Buffers{ext}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Regular Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that will be processed as-is (no buffering needed)\n",
    "regular_files = [\n",
    "    params.strCommunitiesOut,\n",
    "    params.strCentersOut,\n",
    "    params.strTAZwithATOOut,\n",
    "    params.strQOZOut,\n",
    "    params.strParkAccessibilityOut,\n",
    "    params.strParcelsOut\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading: Communities.geojson\n",
      "   ✅ 75 features loaded\n",
      "📂 Loading: Centers.geojson\n",
      "   ✅ 351 features loaded\n",
      "📂 Loading: TAZWithATOScores.geojson\n",
      "   ✅ 2858 features loaded\n",
      "📂 Loading: UtahQualifiedOpportunityZones.geojson\n",
      "   ✅ 28 features loaded\n",
      "📂 Loading: ParksAndOpenSpace.geojson\n",
      "   🔧 Fixing 1 invalid geometries\n",
      "   ✅ 1 features loaded\n",
      "📂 Loading: Parcels.geojson\n",
      "   🔧 Fixing 358 invalid geometries\n",
      "   ✅ 712235 features loaded\n",
      "✅ Processed 6 regular files\n"
     ]
    }
   ],
   "source": [
    "regular_layers = {}\n",
    "\n",
    "for file_path in regular_files:\n",
    "    name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    regular_layers[name] = load_and_validate(file_path)\n",
    "\n",
    "print(f\"✅ Processed {len(regular_layers)} regular files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Files That Need Buffering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that need buffering (points/lines → polygons)\n",
    "files_to_buffer = [\n",
    "    params.strInterchangesOut_Cur,\n",
    "    params.strInterchangesOut_Fut,\n",
    "    params.strTransitOut_Lcl,\n",
    "    params.strTransitOut_BrtCur,\n",
    "    params.strTransitOut_BrtFut,\n",
    "    params.strTransitOut_LrtCur,\n",
    "    params.strTransitOut_LrtFut,\n",
    "    params.strTransitOut_CrtCur,\n",
    "    params.strTransitOut_CrtFut,\n",
    "    params.strChildCareOut,\n",
    "    params.strHealthCareOut,\n",
    "    params.strSchoolsOut_RegPub,\n",
    "    params.strSchoolsOut_HighEd,\n",
    "    params.strGroceryOut,\n",
    "    params.strCommunityCenterOut,\n",
    "    params.strPathsOut_Cur,\n",
    "    params.strPathsOut_Fut,\n",
    "    params.strATCycleTracksOut_Cur,\n",
    "    params.strATCycleTracksOut_Fut\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading: Interchanges_Buffers.geojson\n",
      "   ✅ 984 features loaded\n",
      "📂 Loading: Interchanges_Future_Buffers.geojson\n",
      "   ✅ 96 features loaded\n",
      "📂 Loading: LocalBusStops_Buffers.geojson\n",
      "   ✅ 5299 features loaded\n",
      "📂 Loading: BRTStops_Buffers.geojson\n",
      "   ✅ 40 features loaded\n",
      "📂 Loading: BRTStops_Future_Buffers.geojson\n",
      "   ✅ 118 features loaded\n",
      "📂 Loading: LRTStops_Buffers.geojson\n",
      "   ✅ 56 features loaded\n",
      "📂 Loading: LRTStops_Future_Buffers.geojson\n",
      "   ✅ 73 features loaded\n",
      "📂 Loading: CRTStops_Buffers.geojson\n",
      "   ✅ 15 features loaded\n",
      "📂 Loading: CRTStops_Future_Buffers.geojson\n",
      "   ✅ 9 features loaded\n",
      "📂 Loading: ChildCare_Buffers.geojson\n",
      "   ✅ 740 features loaded\n",
      "📂 Loading: HealthCare_Buffers.geojson\n",
      "   ✅ 472 features loaded\n",
      "📂 Loading: SchoolsRegPublic_Buffers.geojson\n",
      "   ✅ 2060 features loaded\n",
      "📂 Loading: SchoolsHigherEd_Buffers.geojson\n",
      "   ✅ 130 features loaded\n",
      "📂 Loading: GroceryStores_Buffers.geojson\n",
      "   ✅ 874 features loaded\n",
      "📂 Loading: CommunityCenter_Buffers.geojson\n",
      "   ✅ 350 features loaded\n",
      "📂 Loading: ATPaths_Buffers.geojson\n",
      "   ✅ 16064 features loaded\n",
      "📂 Loading: ATPaths_Future_Buffers.geojson\n",
      "   ✅ 1378 features loaded\n",
      "📂 Loading: ATCycleTracks_Buffers.geojson\n",
      "   ✅ 90 features loaded\n",
      "📂 Loading: ATCycleTracks_Future_Buffers.geojson\n",
      "   ✅ 1574 features loaded\n",
      "✅ Processed 19 buffered files\n"
     ]
    }
   ],
   "source": [
    "buffered_layers = {}\n",
    "\n",
    "for file_path in files_to_buffer:\n",
    "    # Create buffered filename\n",
    "    buffered_path = add_buffer_suffix(file_path)\n",
    "    name = os.path.splitext(os.path.basename(buffered_path))[0]\n",
    "\n",
    "    buffered_layers[name] = load_and_clean(buffered_path)\n",
    "\n",
    "print(f\"✅ Processed {len(buffered_layers)} buffered files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Total layers ready for analysis: 25\n",
      "\n",
      "📊 Layer Summary:\n",
      "   Communities: 75 features, CRS: EPSG:26912\n",
      "   Centers: 351 features, CRS: EPSG:26912\n",
      "   TAZWithATOScores: 2858 features, CRS: EPSG:26912\n",
      "   UtahQualifiedOpportunityZones: 28 features, CRS: EPSG:26912\n",
      "   ParksAndOpenSpace: 1 features, CRS: EPSG:26912\n",
      "   Parcels: 712235 features, CRS: EPSG:26912\n",
      "   Interchanges_Buffers: 984 features, CRS: EPSG:26912\n",
      "   Interchanges_Future_Buffers: 96 features, CRS: EPSG:26912\n",
      "   LocalBusStops_Buffers: 5299 features, CRS: EPSG:26912\n",
      "   BRTStops_Buffers: 40 features, CRS: EPSG:26912\n",
      "   BRTStops_Future_Buffers: 118 features, CRS: EPSG:26912\n",
      "   LRTStops_Buffers: 56 features, CRS: EPSG:26912\n",
      "   LRTStops_Future_Buffers: 73 features, CRS: EPSG:26912\n",
      "   CRTStops_Buffers: 15 features, CRS: EPSG:26912\n",
      "   CRTStops_Future_Buffers: 9 features, CRS: EPSG:26912\n",
      "   ChildCare_Buffers: 740 features, CRS: EPSG:26912\n",
      "   HealthCare_Buffers: 472 features, CRS: EPSG:26912\n",
      "   SchoolsRegPublic_Buffers: 2060 features, CRS: EPSG:26912\n",
      "   SchoolsHigherEd_Buffers: 130 features, CRS: EPSG:26912\n",
      "   GroceryStores_Buffers: 874 features, CRS: EPSG:26912\n",
      "   CommunityCenter_Buffers: 350 features, CRS: EPSG:26912\n",
      "   ATPaths_Buffers: 16064 features, CRS: EPSG:26912\n",
      "   ATPaths_Future_Buffers: 1378 features, CRS: EPSG:26912\n",
      "   ATCycleTracks_Buffers: 90 features, CRS: EPSG:26912\n",
      "   ATCycleTracks_Future_Buffers: 1574 features, CRS: EPSG:26912\n"
     ]
    }
   ],
   "source": [
    "# Combine all processed layers\n",
    "all_layers = {}\n",
    "all_layers.update(regular_layers)\n",
    "all_layers.update(buffered_layers)\n",
    "\n",
    "print(f\"🎯 Total layers ready for analysis: {len(all_layers)}\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\n📊 Layer Summary:\")\n",
    "for name, gdf in all_layers.items():\n",
    "    print(f\"   {name}: {len(gdf)} features, CRS: {gdf.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Union Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Union Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_two_layers(gdf1, gdf2):\n",
    "    # Ensure both layers have the same coordinate system\n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "    # Combine both layers into single GeoDataFrame\n",
    "    gdf_combined = gpd.GeoDataFrame(pd.concat([gdf1, gdf2], ignore_index=True), crs=gdf1.crs)\n",
    "    # Perform geometric union of all geometries\n",
    "    unioned_geom = unary_union(gdf_combined.geometry)\n",
    "    # Return as new GeoDataFrame\n",
    "    return gpd.GeoDataFrame(geometry=[unioned_geom], crs=gdf_combined.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sequential_union(named_layers):\n",
    "#     \"\"\"\n",
    "#     Recursively union layers in pairs to avoid memory issues.\n",
    "#     Uses tournament bracket approach: A+B, C+D, then AB+CD, etc.\n",
    "#     \"\"\"\n",
    "#     # Base case: if only one layer remains, return it\n",
    "#     if len(named_layers) == 1:\n",
    "#         return named_layers[0][1]\n",
    "\n",
    "#     # Process layers in pairs\n",
    "#     new_layers = []\n",
    "#     for i in range(0, len(named_layers), 2):\n",
    "#         if i + 1 < len(named_layers):\n",
    "#             # Union two layers\n",
    "#             (name1, gdf1), (name2, gdf2) = named_layers[i], named_layers[i + 1]\n",
    "\n",
    "#             # Perform spatial union using GeoPandas overlay\n",
    "#             # Set keep_geom_type=False to retain all geometry types from union\n",
    "#             result = union_two_layers(gdf1, gdf2)\n",
    "\n",
    "#             # Create combined name for tracking\n",
    "#             new_name = f\"{name1}+{name2}\"\n",
    "#             new_layers.append((new_name, result))\n",
    "#         else:\n",
    "#             # Odd layer out - carry forward to next round\n",
    "#             new_layers.append(named_layers[i])\n",
    "\n",
    "#     # Recursively process the results until one layer remains\n",
    "#     return sequential_union(new_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_union(named_layers):\n",
    "    \"\"\"\n",
    "    Recursively union layers in pairs using parallel processing.\n",
    "    Uses tournament bracket approach: A+B, C+D, then AB+CD, etc.\n",
    "    \"\"\"\n",
    "    # Base case: if only one layer remains, return it\n",
    "    if len(named_layers) == 1:\n",
    "        return named_layers[0][1]\n",
    "\n",
    "    # Prepare pairs for parallel processing\n",
    "    pairs = []\n",
    "    unpaired = None\n",
    "\n",
    "    for i in range(0, len(named_layers), 2):\n",
    "        if i + 1 < len(named_layers):\n",
    "            # Create pairs for union\n",
    "            pairs.append((named_layers[i], named_layers[i + 1]))\n",
    "        else:\n",
    "            # Odd layer out - carry forward to next round\n",
    "            unpaired = named_layers[i]\n",
    "\n",
    "    # Process pairs in parallel\n",
    "    new_layers = []\n",
    "    if pairs:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(union_pair, pairs))\n",
    "            new_layers.extend(results)\n",
    "\n",
    "    # Add unpaired layer if exists\n",
    "    if unpaired:\n",
    "        new_layers.append(unpaired)\n",
    "\n",
    "    # Recursively process the results until one layer remains\n",
    "    return sequential_union(new_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all_layers dict to list of tuples for processing\n",
    "layers_list = [(name, gdf) for name, gdf in all_layers.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pukar.Bhandari\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 213 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n",
      "C:\\Users\\Pukar.Bhandari\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 9 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    }
   ],
   "source": [
    "gdf_final = sequential_union(layers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_final = gdf_final.to_crs(epsg=4326)\n",
    "final_path = os.path.join(params.dirIntermediate, \"UNIONED.geojson\")\n",
    "gdf_final.to_file(final_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Performing sequential spatial union (excluding Parcels)...\n",
      "⚙️ Unioning round 1, pair 0: Communities + Centers\n",
      "   ↳ Features: 75 + 351\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_Communities+Centers.geojson\n",
      "⚙️ Unioning round 1, pair 1: TAZWithATOScores + Interchanges_Buffers\n",
      "   ↳ Features: 2858 + 984\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_TAZWithATOScores+Interchanges_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 2: Interchanges_Future_Buffers + LocalBusStops_Buffers\n",
      "   ↳ Features: 96 + 5299\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_Interchanges_Future_Buffers+LocalBusStops_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 3: BRTStops_Buffers + BRTStops_Future_Buffers\n",
      "   ↳ Features: 40 + 118\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_BRTStops_Buffers+BRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 4: LRTStops_Buffers + LRTStops_Future_Buffers\n",
      "   ↳ Features: 56 + 73\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_LRTStops_Buffers+LRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 5: CRTStops_Buffers + CRTStops_Future_Buffers\n",
      "   ↳ Features: 15 + 9\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_CRTStops_Buffers+CRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 6: ChildCare_Buffers + HealthCare_Buffers\n",
      "   ↳ Features: 740 + 472\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_ChildCare_Buffers+HealthCare_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 7: SchoolsRegPublic_Buffers + SchoolsHigherEd_Buffers\n",
      "   ↳ Features: 2060 + 130\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_SchoolsRegPublic_Buffers+SchoolsHigherEd_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 8: GroceryStores_Buffers + CommunityCenter_Buffers\n",
      "   ↳ Features: 874 + 350\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_GroceryStores_Buffers+CommunityCenter_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 9: ATPaths_Buffers + ATPaths_Future_Buffers\n",
      "   ↳ Features: 16064 + 1378\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_ATPaths_Buffers+ATPaths_Future_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 10: ATCycleTracks_Buffers + ATCycleTracks_Future_Buffers\n",
      "   ↳ Features: 90 + 1574\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_ATCycleTracks_Buffers+ATCycleTracks_Future_Buffers.geojson\n",
      "⚙️ Unioning round 1, pair 11: UtahQualifiedOpportunityZones + ParksAndOpenSpace\n",
      "   ↳ Features: 28 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_UtahQualifiedOpportunityZones+ParksAndOpenSpace.geojson\n",
      "⚙️ Unioning round 2, pair 0: Communities.geojson+Centers + TAZWithATOScores.geojson+Interchanges_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_Communities.geojson+Centers+TAZWithATOScores.geojson+Interchanges_Buffers.geojson\n",
      "⚙️ Unioning round 2, pair 1: Interchanges_Future_Buffers.geojson+LocalBusStops_Buffers + BRTStops_Buffers.geojson+BRTStops_Future_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_Interchanges_Future_Buffers.geojson+LocalBusStops_Buffers+BRTStops_Buffers.geojson+BRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 2, pair 2: LRTStops_Buffers.geojson+LRTStops_Future_Buffers + CRTStops_Buffers.geojson+CRTStops_Future_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_LRTStops_Buffers.geojson+LRTStops_Future_Buffers+CRTStops_Buffers.geojson+CRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 2, pair 3: ChildCare_Buffers.geojson+HealthCare_Buffers + SchoolsRegPublic_Buffers.geojson+SchoolsHigherEd_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_ChildCare_Buffers.geojson+HealthCare_Buffers+SchoolsRegPublic_Buffers.geojson+SchoolsHigherEd_Buffers.geojson\n",
      "⚙️ Unioning round 2, pair 4: GroceryStores_Buffers.geojson+CommunityCenter_Buffers + ATPaths_Buffers.geojson+ATPaths_Future_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_GroceryStores_Buffers.geojson+CommunityCenter_Buffers+ATPaths_Buffers.geojson+ATPaths_Future_Buffers.geojson\n",
      "⚙️ Unioning round 2, pair 5: ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers + UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace.geojson\n",
      "⚙️ Unioning round 3, pair 0: Communities.geojson+Centers.geojson+TAZWithATOScores.geojson+Interchanges_Buffers + Interchanges_Future_Buffers.geojson+LocalBusStops_Buffers.geojson+BRTStops_Buffers.geojson+BRTStops_Future_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_Communities.geojson+Centers.geojson+TAZWithATOScores.geojson+Interchanges_Buffers+Interchanges_Future_Buffers.geojson+LocalBusStops_Buffers.geojson+BRTStops_Buffers.geojson+BRTStops_Future_Buffers.geojson\n",
      "⚙️ Unioning round 3, pair 1: LRTStops_Buffers.geojson+LRTStops_Future_Buffers.geojson+CRTStops_Buffers.geojson+CRTStops_Future_Buffers + ChildCare_Buffers.geojson+HealthCare_Buffers.geojson+SchoolsRegPublic_Buffers.geojson+SchoolsHigherEd_Buffers\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "✅ Saved intermediate: intermediate_union_LRTStops_Buffers.geojson+LRTStops_Future_Buffers.geojson+CRTStops_Buffers.geojson+CRTStops_Future_Buffers+ChildCare_Buffers.geojson+HealthCare_Buffers.geojson+SchoolsRegPublic_Buffers.geojson+SchoolsHigherEd_Buffers.geojson\n",
      "⚙️ Unioning round 3, pair 2: GroceryStores_Buffers.geojson+CommunityCenter_Buffers.geojson+ATPaths_Buffers.geojson+ATPaths_Future_Buffers + ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers.geojson+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace\n",
      "   ↳ Features: 1 + 1\n",
      "⚙️ Performing unary_union with pre-checks...\n",
      "❌ Failed union in round 3 pair 2: Failed to create GeoJSON datasource: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_GroceryStores_Buffers.geojson+CommunityCenter_Buffers.geojson+ATPaths_Buffers.geojson+ATPaths_Future_Buffers+ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers.geojson+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace.geojson: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_Communities+Centers.geojson: No such file or directory\n"
     ]
    },
    {
     "ename": "DataSourceError",
     "evalue": "Failed to create GeoJSON datasource: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_GroceryStores_Buffers.geojson+CommunityCenter_Buffers.geojson+ATPaths_Buffers.geojson+ATPaths_Future_Buffers+ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers.geojson+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace.geojson: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_Communities+Centers.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:1947\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_create\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpyogrio\\\\_err.pyx:183\u001b[0m, in \u001b[0;36mpyogrio._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: Failed to create GeoJSON datasource: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_GroceryStores_Buffers.geojson+CommunityCenter_Buffers.geojson+ATPaths_Buffers.geojson+ATPaths_Future_Buffers+ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers.geojson+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace.geojson: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_Communities+Centers.geojson: No such file or directory\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[7], line 131\u001b[0m\n",
      "\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# --- Execute Union ---\u001b[39;00m\n",
      "\u001b[0;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚙️ Performing sequential spatial union (excluding Parcels)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m--> 131\u001b[0m gdfUnioned \u001b[38;5;241m=\u001b[39m \u001b[43msequential_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother_layers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# --- Union with Parcels separately ---\u001b[39;00m\n",
      "\u001b[0;32m    134\u001b[0m parcels_name, parcels_gdf \u001b[38;5;241m=\u001b[39m parcels_layer\n",
      "\n",
      "Cell \u001b[1;32mIn[7], line 114\u001b[0m, in \u001b[0;36msequential_union\u001b[1;34m(named_layers, round_num)\u001b[0m\n",
      "\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    112\u001b[0m         new_layers\u001b[38;5;241m.\u001b[39mappend(named_layers[i])\n",
      "\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msequential_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[1;32mIn[7], line 114\u001b[0m, in \u001b[0;36msequential_union\u001b[1;34m(named_layers, round_num)\u001b[0m\n",
      "\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    112\u001b[0m         new_layers\u001b[38;5;241m.\u001b[39mappend(named_layers[i])\n",
      "\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msequential_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[1;32mIn[7], line 105\u001b[0m, in \u001b[0;36msequential_union\u001b[1;34m(named_layers, round_num)\u001b[0m\n",
      "\u001b[0;32m    102\u001b[0m pair_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 105\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mload_if_exists_or_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    106\u001b[0m     new_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    107\u001b[0m     new_layers\u001b[38;5;241m.\u001b[39mappend((new_name, result))\n",
      "\n",
      "Cell \u001b[1;32mIn[7], line 89\u001b[0m, in \u001b[0;36mload_if_exists_or_union\u001b[1;34m(gdf1, gdf2, round_num, pair_idx, name1, name2)\u001b[0m\n",
      "\u001b[0;32m     86\u001b[0m     gdf2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gdf2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbuffer(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;32m     87\u001b[0m     result \u001b[38;5;241m=\u001b[39m union_layers_pairwise(gdf1, gdf2)\n",
      "\u001b[1;32m---> 89\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeoJSON\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved intermediate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\geopandas\\geodataframe.py:1637\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[1;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1543\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n",
      "\u001b[0;32m   1544\u001b[0m \n",
      "\u001b[0;32m   1545\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   1633\u001b[0m \n",
      "\u001b[0;32m   1634\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_file\n",
      "\u001b[1;32m-> 1637\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\geopandas\\io\\file.py:731\u001b[0m, in \u001b[0;36m_to_file\u001b[1;34m(df, filename, driver, schema, index, mode, crs, engine, metadata, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m--> 731\u001b[0m     \u001b[43m_to_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m    733\u001b[0m     _to_file_fiona(df, filename, driver, schema, crs, mode, metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\geopandas\\io\\file.py:793\u001b[0m, in \u001b[0;36m_to_file_pyogrio\u001b[1;34m(df, filename, driver, schema, crs, mode, metadata, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n",
      "\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoDataFrame cannot contain duplicated column names.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m--> 793\u001b[0m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\pyogrio\\geopandas.py:662\u001b[0m, in \u001b[0;36mwrite_dataframe\u001b[1;34m(df, path, layer, driver, encoding, geometry_type, promote_to_multi, nan_as_null, append, use_arrow, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m    660\u001b[0m     geometry \u001b[38;5;241m=\u001b[39m to_wkb(geometry\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;32m--> 662\u001b[0m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\wfrc\\Lib\\site-packages\\pyogrio\\raw.py:723\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(path, geometry, field_data, fields, field_mask, layer, driver, geometry_type, crs, encoding, promote_to_multi, nan_as_null, append, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, gdal_tz_offsets, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# preprocess kwargs and split in dataset and layer creation options\u001b[39;00m\n",
      "\u001b[0;32m    719\u001b[0m dataset_kwargs, layer_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_kwargs(\n",
      "\u001b[0;32m    720\u001b[0m     driver, dataset_options, layer_options, kwargs\n",
      "\u001b[0;32m    721\u001b[0m )\n",
      "\u001b[1;32m--> 723\u001b[0m \u001b[43mogr_write\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:2307\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_write\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:2135\u001b[0m, in \u001b[0;36mpyogrio._io.create_ogr_dataset_layer\u001b[1;34m()\u001b[0m\n",
      "\n",
      "File \u001b[1;32mpyogrio\\\\_io.pyx:1956\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_create\u001b[1;34m()\u001b[0m\n",
      "\n",
      "\u001b[1;31mDataSourceError\u001b[0m: Failed to create GeoJSON datasource: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_GroceryStores_Buffers.geojson+CommunityCenter_Buffers.geojson+ATPaths_Buffers.geojson+ATPaths_Future_Buffers+ATCycleTracks_Buffers.geojson+ATCycleTracks_Future_Buffers.geojson+UtahQualifiedOpportunityZones.geojson+ParksAndOpenSpace.geojson: d:\\GitHub\\MAP-Housing-ATO-Calculator\\intermediate\\intermediate_union_Communities+Centers.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Cell 2: Perform the union and export final output\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.validation import make_valid\n",
    "from shapely.ops import unary_union\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.validation import explain_validity\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_geometries(gdf):\n",
    "    gdf = gdf[~gdf[\"geometry\"].is_empty & gdf[\"geometry\"].notna()]  # Drop empties and Nones\n",
    "    gdf = gdf[gdf.is_valid]  # Drop invalid geometries\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].buffer(0)  # Force topology repair\n",
    "    return gdf\n",
    "\n",
    "def debug_union_stepwise(gdf):\n",
    "    gdf = gdf.explode(index_parts=False).reset_index(drop=True)\n",
    "    failed = []\n",
    "    current = None\n",
    "\n",
    "    for idx, geom in enumerate(gdf.geometry):\n",
    "        if current is None:\n",
    "            current = geom\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            current = current.union(geom)\n",
    "        except GEOSException as e:\n",
    "            print(f\"❌ Failed at geometry index {idx}: {e}\")\n",
    "            failed.append((idx, geom))\n",
    "\n",
    "    return failed\n",
    "\n",
    "def union_layers_pairwise(gdf1, gdf2):\n",
    "    # Ensure same CRS\n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "\n",
    "    # Combine and clean\n",
    "    gdf_combined = gpd.GeoDataFrame(pd.concat([gdf1, gdf2], ignore_index=True), crs=gdf1.crs)\n",
    "    gdf_combined = gdf_combined[~gdf_combined[\"geometry\"].is_empty & gdf_combined[\"geometry\"].notna()]\n",
    "    gdf_combined = gdf_combined.explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"⚙️ Performing unary_union with pre-checks...\")\n",
    "\n",
    "    try:\n",
    "        unioned_geom = unary_union(gdf_combined.geometry)\n",
    "    except GEOSException as e:\n",
    "        print(f\"❌ unary_union failed: {e}\")\n",
    "        print(\"🔁 Falling back to stepwise union for debugging...\")\n",
    "\n",
    "        failed_geoms = debug_union_stepwise(gdf_combined)\n",
    "        if failed_geoms:\n",
    "            print(f\"⚠️ Found {len(failed_geoms)} geometries causing union errors.\")\n",
    "            debug_gdf = gpd.GeoDataFrame({'geometry': [g for _, g in failed_geoms]}, crs=gdf_combined.crs)\n",
    "            debug_gdf.to_file(\"debug_union_failures.geojson\", driver=\"GeoJSON\")\n",
    "            print(\"🧯 Offending geometries saved to debug_union_failures.geojson\")\n",
    "        raise ValueError(\"Stepwise union identified invalid geometries. See debug_union_failures.geojson.\")\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=[unioned_geom], crs=gdf_combined.crs)\n",
    "\n",
    "def load_if_exists_or_union(gdf1, gdf2, round_num, pair_idx, name1, name2):\n",
    "    base1 = os.path.splitext(name1)[0]\n",
    "    base2 = os.path.splitext(name2)[0]\n",
    "    filename = f\"intermediate_union_{base1}+{base2}.geojson\"\n",
    "    filepath = os.path.join(dirIntermediate, filename)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"🔁 Using cached union result: {filename}\")\n",
    "        return gpd.read_file(filepath)\n",
    "\n",
    "    print(f\"⚙️ Unioning round {round_num}, pair {pair_idx}: {base1} + {base2}\")\n",
    "    print(f\"   ↳ Features: {len(gdf1)} + {len(gdf2)}\")\n",
    "\n",
    "    # Clean geometries\n",
    "    gdf1 = clean_geometries(gdf1)\n",
    "    gdf2 = clean_geometries(gdf2)\n",
    "\n",
    "    try:\n",
    "        result = union_layers_pairwise(gdf1, gdf2)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Attempting fallback: buffering geometries with 0...\")\n",
    "        gdf1[\"geometry\"] = gdf1[\"geometry\"].buffer(0)\n",
    "        gdf2[\"geometry\"] = gdf2[\"geometry\"].buffer(0)\n",
    "        result = union_layers_pairwise(gdf1, gdf2)\n",
    "\n",
    "    result.to_file(filepath, driver=\"GeoJSON\")\n",
    "    print(f\"✅ Saved intermediate: {filename}\")\n",
    "    return result\n",
    "\n",
    "def sequential_union(named_layers, round_num=1):\n",
    "    if len(named_layers) == 1:\n",
    "        return named_layers[0][1]\n",
    "\n",
    "    new_layers = []\n",
    "\n",
    "    for i in range(0, len(named_layers), 2):\n",
    "        if i + 1 < len(named_layers):\n",
    "            (name1, gdf1), (name2, gdf2) = named_layers[i], named_layers[i + 1]\n",
    "            pair_idx = i // 2\n",
    "\n",
    "            try:\n",
    "                result = load_if_exists_or_union(gdf1, gdf2, round_num, pair_idx, name1, name2)\n",
    "                new_name = f\"{name1}+{name2}\"\n",
    "                new_layers.append((new_name, result))\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed union in round {round_num} pair {pair_idx}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            new_layers.append(named_layers[i])\n",
    "\n",
    "    return sequential_union(new_layers, round_num + 1)\n",
    "\n",
    "# --- Split out Parcels layer ---\n",
    "parcels_layer = None\n",
    "other_layers = []\n",
    "\n",
    "for name, gdf in layers:\n",
    "    if \"Parcels\" in name:\n",
    "        parcels_layer = (name, gdf)\n",
    "    else:\n",
    "        other_layers.append((name, gdf))\n",
    "\n",
    "if parcels_layer is None:\n",
    "    raise ValueError(\"❌ Could not find a layer containing 'Parcels' in the name.\")\n",
    "\n",
    "# --- Execute Union ---\n",
    "print(\"⚙️ Performing sequential spatial union (excluding Parcels)...\")\n",
    "gdfUnioned = sequential_union(other_layers)\n",
    "\n",
    "# --- Union with Parcels separately ---\n",
    "parcels_name, parcels_gdf = parcels_layer\n",
    "print(f\"🔀 Final union with {parcels_name}...\")\n",
    "gdfUnioned = union_layers_pairwise(gdfUnioned, parcels_gdf)\n",
    "\n",
    "# --- Reproject and Export ---\n",
    "gdfUnioned = gdfUnioned.to_crs(epsg=4326)\n",
    "\n",
    "final_path = os.path.join(dirIntermediate, \"UNIONED.geojson\")\n",
    "gdfUnioned.to_file(final_path, driver=\"GeoJSON\")\n",
    "print(f\"✅ Final unioned layer written to: {final_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "march2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
